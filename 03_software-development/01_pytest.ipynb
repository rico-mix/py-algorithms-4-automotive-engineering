{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Table of contents](../toc.ipynb)\n",
    "\n",
    "<img src=\"https://docs.pytest.org/en/latest/_static/pytest1.png\" alt=\"pytest\" width=\"150\" align=\"right\">\n",
    "\n",
    "\n",
    "# Pytest\n",
    "\n",
    "Software rots with time and to keep the software in a healthy state tests are vital.\n",
    "\n",
    "Pytest is a testing library for Python code and [software testing](https://en.wikipedia.org/wiki/Software_testing) is very important to ensure:\n",
    "* quality of software,\n",
    "* maintainability,\n",
    "* flexibility.\n",
    "\n",
    "Imagine a large piece of software which is not tested and there is just one super guru developer who knows how it works and how to add a feature. This is a nightmare for everyone, the company, the dev team, the managers. It will be very hard to change or extend the software.\n",
    "\n",
    "With tests, you are much more confident to change and improve the implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Alternatives\n",
    "\n",
    "Pytest is just one option of test frameworks for Python code. You can also look for\n",
    "* doctest,\n",
    "* unittest,\n",
    "* nose.\n",
    "\n",
    "Using pytest is just my preference, the important thing is that you care about software testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## A bit of background\n",
    "\n",
    "As said, an untested software becomes rigid and is actually not soft anymore. If you have no tests, developers will be afraid to apply changes to the code and hence the software quality decreases over time. \n",
    "\n",
    "If you skip testing and prefer fast implementation you are accepting something which is called **[technical debt](https://en.wikipedia.org/wiki/Technical_debt)**. \n",
    "\n",
    "It is known from many examples in practice that high technical debt during implementation will cause higher costs later than taking care about software quality from the first day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The function cost of change over time is roughly exponential as shown in next figure. Hence it makes sense to detect and resolve errors as early as possible.\n",
    "\n",
    "<img src=\"cost_of_change.svg\" alt=\"git\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A test is not a proof\n",
    "\n",
    "There is one common misunderstanding in testing software. With a test, you can **only say that you could not find an error**, but **you can not say that the software is correct** (free of errors).\n",
    "\n",
    "There might still be errors which another test could discover.\n",
    "\n",
    "The only concept which proof correctness is [formal verification](https://en.wikipedia.org/wiki/Formal_verification), which is a mathematical procedure to check is software fulfills specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### But testing is better than no test\n",
    "\n",
    "Despite the inability of tests to proof correctness, testing is state of the art in software development.\n",
    "\n",
    "There are different layers of tests, beginning from \n",
    "* unittesting,\n",
    "* to integration testing,\n",
    "* to system testing.\n",
    "\n",
    "There is much more theory to discover in testing software but we will move now to the practical part and learn how to use pytest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Processes for software tests\n",
    "\n",
    "Looking from the process view on software testing there are different concepts at which stage developers write software tests.\n",
    "\n",
    "* Never. This concept should be avoided of course but you will be surprised how often teams develop software without tests in practice.\n",
    "* [Test driven development (TDD)](https://en.wikipedia.org/wiki/Test-driven_development). This is the extreme counter concept to *never* where fast repetition cycles of: add test; run tests to see that new test fails; write code; run tests; refactor; repeat are conducted.\n",
    "* [V-Model](https://en.wikipedia.org/wiki/V-Model). This classical procedure form product engineering can be summarized as: code, write tests, verify and validate the implementation. It is in between *never* and *TDD*.\n",
    "\n",
    "Apart from the method *never*, TDD and V-Model can be applied. However, the V-Model takes care about test at a later stage when usually time to market is close and removal of errors high. Hence, continuous testing and a now or never mindset should be fostered in V-Model from my point of view."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Running the first pytest\n",
    "\n",
    "Pytest is mainly a command line tool and collects and executes all files and function with `test_` prefix. Hence, we need a test file before we can use pytest.\n",
    "\n",
    "The file `test_simple_test` contains:\n",
    "\n",
    "```python\n",
    "def test_a_simple_test():\n",
    "    assert(1 == 1)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Once you have installed pytest as package in your python environment you can run pytest in terminal with `pytest`.\n",
    "\n",
    "Here in jupyter we will use the `!` as prefix to tell jupyter to run commands in command line. \n",
    "\n",
    "So let us try and run pytest for the specific file `test_simple_test.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.7.6, pytest-6.1.1, py-1.9.0, pluggy-0.13.1 -- /Users/Rico/opt/miniconda3/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /Users/Rico/Desktop/Master/Master 2/Python Algorithmus FZT/py-algorithms-4-automotive-engineering, configfile: pytest.ini\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "test_simple_test.py::test_a_simple_test \u001b[32mPASSED\u001b[0m\u001b[32m                           [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pytest --verbose test_simple_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The output shows that one file was collected and that the test passed.\n",
    "\n",
    "However, this was quite clear because the test will always pass due to `assert(1 == 1)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Testing a function\n",
    "\n",
    "We will test now the function\n",
    "\n",
    "```python\n",
    "def add_func(a, b):\n",
    "    return a + b  \n",
    "```\n",
    "which is contained in the file `my_source.py`\n",
    "\n",
    "The respective test is \n",
    "```python\n",
    "def test_add_one():\n",
    "    assert add_func(2, 5) == 7\n",
    "\n",
    "```\n",
    "in `test_my_source.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.7.6, pytest-6.1.1, py-1.9.0, pluggy-0.13.1 -- /Users/Rico/opt/miniconda3/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /Users/Rico/Desktop/Master/Master 2/Python Algorithmus FZT/py-algorithms-4-automotive-engineering, configfile: pytest.ini\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "test_my_source.py::test_add_one \u001b[32mPASSED\u001b[0m\u001b[32m                                   [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pytest --verbose test_my_source.py::test_add_one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The syntax `pytest --verbose test_my_source.py::test_add_one` told pytest to run one specific test function in one specific file.\n",
    "\n",
    "Now let us have a look at a failing test. The test function \n",
    "\n",
    "```python\n",
    "def test_add_fail():\n",
    "    assert add_func(5, 5) == 9\n",
    "```\n",
    "in `test_my_source.py` file should fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.7.6, pytest-6.1.1, py-1.9.0, pluggy-0.13.1 -- /Users/Rico/opt/miniconda3/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/Rico/Desktop/Master/Master 2/Python Algorithmus FZT/py-algorithms-4-automotive-engineering, configfile: pytest.ini\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "test_my_source.py::test_add_one \u001b[32mPASSED\u001b[0m\u001b[32m                                   [ 50%]\u001b[0m\n",
      "test_my_source.py::test_add_fail \u001b[31mFAILED\u001b[0m\u001b[31m                                  [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________________________________ test_add_fail _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_add_fail\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m add_func(\u001b[94m5\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m) == \u001b[94m9\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 10 == 9\u001b[0m\n",
      "\u001b[1m\u001b[31mE         +10\u001b[0m\n",
      "\u001b[1m\u001b[31mE         -9\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_my_source.py\u001b[0m:9: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_my_source.py::test_add_fail - assert 10 == 9\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m\u001b[31m in 0.13s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest --verbose test_my_source.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We called now all tests of `test_my_source.py` at once with `pytest --verbose test_my_source.py`. The first test still passes, the second test fails as expected.\n",
    "\n",
    "You can also group tests within classes and the syntax to call a specific test in a class of a file becomes \n",
    "\n",
    "`pytest test_mod.py::TestClass::test_method`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Types of assertions\n",
    "\n",
    "Next to assertions of integers there are assertions for floats, types, warnings, strings, error codes, exceptions, and so forth. Please find in pytest documentation many basic examples [https://docs.pytest.org/en/latest/assert.html#assert](https://docs.pytest.org/en/latest/assert.html#assert).\n",
    "\n",
    "If you want to compare floats, please remember to use relative or absolute tolerance instead of `assert(3.14 == 3.14)`. You can user either the pytest `approx` function [https://docs.pytest.org/en/latest/reference.html#pytest-approx](https://docs.pytest.org/en/latest/reference.html#pytest-approx) or numpy `np.allclose` (see numpy notebook of this course).\n",
    "\n",
    "We will take a look at some float comparisons and skip the other types of assertions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A plain float comparison of two numpy arrays will cause a failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array([0.1, 0.2]) + np.array([0.2, 0.4]) == np.array([0.3, 0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The correct way to compare floats is to use an approximate comparison. The default precision is $1e-6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytest import approx\n",
    "\n",
    "np.array([0.1, 0.2]) + np.array([0.2, 0.4]) == approx(np.array([0.3, 0.6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But the precision can also be passed to the comparer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0.1, 0.2]) + np.array([0.2, 0.4]\n",
    "                                ) == approx(np.array([0.3, 0.6]), abs=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other pytest features\n",
    "\n",
    "There are much more features in pytest which help to write good tests. You can for instance\n",
    "* [parametrize tests](https://docs.pytest.org/en/latest/example/parametrize.html) to test code with a set of stimuli data,\n",
    "* [work with markers and fixtures](https://docs.pytest.org/en/latest/example/markers.html) to select tests through their names or through labels, or to skip test which take long time to conduct,\n",
    "* [check code coverage with `pytest-cov`](https://pytest-cov.readthedocs.io/en/latest/) to see if your tests covered all lines of the code under test.\n",
    "\n",
    "Next, we will briefly try `pytest-cov`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.7.6, pytest-6.1.1, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /Users/Rico/Desktop/Master/Master 2/Python Algorithmus FZT/py-algorithms-4-automotive-engineering/03_software-development, configfile: ov\n",
      "collected 3 items                                                              \u001b[0m\n",
      "\n",
      "test_my_source.py \u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                                     [ 66%]\u001b[0m\n",
      "test_simple_test.py \u001b[32m.\u001b[0m\u001b[31m                                                    [100%]\u001b[0m\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "\u001b[31m\u001b[1m________________________________ test_add_fail _________________________________\u001b[0m\n",
      "\n",
      "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_add_fail\u001b[39;49;00m():\n",
      ">       \u001b[94massert\u001b[39;49;00m add_func(\u001b[94m5\u001b[39;49;00m, \u001b[94m5\u001b[39;49;00m) == \u001b[94m9\u001b[39;49;00m\n",
      "\u001b[1m\u001b[31mE       assert 10 == 9\u001b[0m\n",
      "\u001b[1m\u001b[31mE        +  where 10 = add_func(5, 5)\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[31mtest_my_source.py\u001b[0m:9: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED test_my_source.py::test_add_fail - assert 10 == 9\n",
      "\u001b[31m========================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m2 passed\u001b[0m\u001b[31m in 0.16s\u001b[0m\u001b[31m ==========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pytest -cov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This report is appended with pytest coverage information at the bottom. You can see in a table how many statements are covered by tests. \n",
    "\n",
    "The coverage of `my_source.py` is at 44%, hence there is some not tested code in the file. You can see in the `Missing` column that line 8 and lines 20-23 are not tested.\n",
    "\n",
    "Add to this minimal report, you can also export nicely rendered html reports where the un-covered lines of code are marked in red color."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Books on software testing\n",
    "\n",
    "There are many books on software testing in general and for each programming language out there. Please find here a short list.\n",
    "\n",
    "* Test-Driven Python Development [[Govindaraj2015]](../references.bib).\n",
    "* Chapter 9 of Clean Code [[Martin2008]](../references.bib).\n",
    "* Python Testing with pytest [[Okken2018]](../references.bib)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise:  pytest (10 minutes)\n",
    "\n",
    "<img src=\"../_static/exercise.png\" alt=\"Exercise\" width=\"75\" align=\"left\">\n",
    "\n",
    "* Write a pytest file which checks the implementation of a function we used at beginning of this course\n",
    "\n",
    "```python\n",
    "def euclid(p, q):\n",
    "    \"\"\"Return the euclidean distance.\n",
    "    Args:\n",
    "        p (list): p vector\n",
    "        q (list): q vector\n",
    "\n",
    "    Returns:\n",
    "        euclidean distance\n",
    "    \"\"\"\n",
    "    dist = 0\n",
    "    for p_i, q_i in zip(p, q):\n",
    "        dist += (q_i - p_i) ** 2\n",
    "    return dist ** 0.5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.7.6, pytest-6.1.1, py-1.9.0, pluggy-0.13.1 -- /Users/Rico/opt/miniconda3/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /Users/Rico/Desktop/Master/Master 2/Python Algorithmus FZT/py-algorithms-4-automotive-engineering, configfile: pytest.ini\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "test_euclid.py::test_euclid \u001b[32mPASSED\u001b[0m\u001b[32m                                       [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#own solution\n",
    "\n",
    "! pytest --verbose test_euclid.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solution\n",
    "\n",
    "Please find one possible solution in [`solution_pytest.py`](solution_pytest.py) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform darwin -- Python 3.7.6, pytest-6.1.1, py-1.9.0, pluggy-0.13.1 -- /Users/Rico/opt/miniconda3/bin/python\r\n",
      "cachedir: .pytest_cache\r\n",
      "rootdir: /Users/Rico/Desktop/Master/Master 2/Python Algorithmus FZT/py-algorithms-4-automotive-engineering, configfile: pytest.ini\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "solution_pytest.py::test_euclid \u001b[32mPASSED\u001b[0m\u001b[32m                                   [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pytest --verbose solution_pytest.py"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
